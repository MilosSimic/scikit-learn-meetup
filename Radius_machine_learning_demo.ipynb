{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in this (fictional) dataset contains information about a person who signed up for a demo of a software product. The machine learning objective is to build a classification model to predict whether or not a given user will \"convert\" and purchase the software. You should use the \"train\" dataset to train and cross-validate your models, and the \"test\" dataset to evaluate how well your final model performs.\n",
    "\n",
    "Data Feature Descriptions:\n",
    "\n",
    "<li>num_visits - the number of distinct sessions the user spent with the software</li>\n",
    "<li>total_minutes_on_demo - the number of total minutes the user spent using the software</li>\n",
    "<li>user_age - the age of the user in years</li>\n",
    "<li>demo_source - how the user initially discovered the software</li>\n",
    "<li>data_rows_input - how many rows of their own data the user uploaded to the software</li>\n",
    "<li>users_in_network - how many people the user knows who already use the software</li>\n",
    "<li>outreach_emails_sent - the number of emails sent to the user by the software company while the user is in the demo period</li>\n",
    "<li>support_tickets_filed - the number of support requests sent by the user to the software company while using the demo</li>\n",
    "\n",
    "Label:\n",
    "<li>converted - boolean label of whether the user converted and purchased the software.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda is a distribution of the Python and R programming languages for large-scale data processing, predictive analytics, and scientific computing. The open source version of Anaconda includes support for over 400 of the most popular Python packages for data science, including the packages used in this demo.\n",
    "\n",
    "<li>https://www.continuum.io/why-anaconda</li>\n",
    "<li>package list: https://docs.continuum.io/anaconda/pkg-docs</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the csv data into a Pandas dataframe. Pandas is great for ETL and exploration of tabular data (tsv, csv, psv format) and dictionary (or json) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('meetup_train.csv')\n",
    "test_df = pd.read_csv('meetup_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the feature types and count.  Check for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Objects in this dataset refer to string data types. In Pandas, mixed type columns will also be labeled object.\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get descriptive statistics for the numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The \"converted\" column is the label column - so we now know the overall distribution of wins and losses.\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the number of unique items in the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_features = ['location', 'demo_source', 'operating_system']\n",
    "for c in categorical_features:\n",
    "    n = train_df[c].nunique()\n",
    "    print c, 'has', n, 'unique items'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Visualize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Seaborn statistical data visualization python library\n",
    "https://web.stanford.edu/~mwaskom/software/seaborn/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "sns.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "ax = sns.countplot(x='demo_source', data=train_df, hue='converted', palette='Set2')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='operating_system', data=train_df, hue='converted', palette='Set2')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(\"converted\", col=\"location\", col_wrap=6, data=train_df, \n",
    "               kind=\"count\", size=2.9, aspect=.8, palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"total_minutes_on_demo\", y=\"demo_source\", \n",
    "                   hue=\"converted\", row=\"operating_system\",\n",
    "                   data=train_df, orient=\"h\", size=2.5, aspect=3.5, palette=\"Set2\",\n",
    "                   kind=\"violin\", split=True, bw=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"data_rows_input\", y=\"demo_source\", \n",
    "                   hue=\"converted\", row=\"operating_system\",\n",
    "                   data=train_df, orient=\"h\", size=2.5, aspect=3.5, palette=\"Set2\",\n",
    "                   kind=\"violin\", split=True, bw=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_context(context=\"notebook\")\n",
    "g = sns.pairplot(train_df[['num_visits', 'total_minutes_on_demo', 'user_age',\n",
    "                           'data_rows_input', 'users_in_network', 'outreach_emails_sent',\n",
    "                           'support_tickets_filed']], size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_context(context=\"talk\")\n",
    "ax = sns.jointplot(x=\"users_in_network\", y=\"user_age\", data=train_df, kind=\"scatter\", size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert the \"converted\" label column to numpy arrays\n",
    "train_converted = train_df.pop('converted').values\n",
    "test_converted = test_df.pop('converted').values\n",
    "\n",
    "# drop location (not informative) features from training and test dataframes\n",
    "train_df.drop('location', axis=1, inplace=True)\n",
    "test_df.drop('location', axis=1, inplace=True)\n",
    "\n",
    "# transform the categorical features to binary features\n",
    "train_dummies_df = pd.get_dummies(train_df)\n",
    "test_dummies_df = pd.get_dummies(test_df)\n",
    "\n",
    "# get the feature names - this will be useful for the model visualization and feature analysis\n",
    "features = train_dummies_df.columns.values\n",
    "\n",
    "# convert the training and test dataframes to numpy arrays\n",
    "train_data = train_dummies_df.values\n",
    "test_data = test_dummies_df.values\n",
    "\n",
    "print 'training data shape', train_data.shape\n",
    "print 'test data shape', test_data.shape\n",
    "print 'converted label data shape', train_converted.shape\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dummies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dummies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use classification tools in scikit-learn to make predictions.\n",
    "\n",
    "Popular classification methods:\n",
    "<li>Decision Tree</li>\n",
    "<li>Logistic Regression</li>\n",
    "<li>Support Vector Machines</li>\n",
    "<li>Ensemble methods: Random Forest, Gradient Tree Boosting, AdaBoost</li>\n",
    "\n",
    "Inputs:\n",
    "<li>X: array of training data with shape = (n_samples, n_features)</li>\n",
    "<li>y: array of labels with shape = (n_samples)</li>\n",
    "<li>optional model parameters</li>\n",
    "\n",
    "Key classifier methods:\n",
    "<li>fit: Fit the model according to the given training data.</li>\n",
    "<li>predict: Predict class labels for samples in X.</li>\n",
    "<li>score: Returns the mean accuracy on the given test data and labels.</li>\n",
    "\n",
    "http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "Benefits:\n",
    "<li>Easily interpretable</li>\n",
    "<li>Fast</li>\n",
    "<li>Can handle both numeric and categorical data</li>\n",
    "<li>Data does not need to be normalized</li>\n",
    "\n",
    "Disadvantages:\n",
    "<li>Prone to overfitting</li>\n",
    "<li>Unstable to small data variations</li>\n",
    "\n",
    "Parameters:\n",
    "<li>criterion: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Default = 'gini'.</li>\n",
    "<li>max_features: The number of features to consider when looking for the best split. Default = None.</li>\n",
    "<li>max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Default = None.</li>\n",
    "<li>min_samples_split: The minimum number of samples required to split an internal node. Default = 2.</li>\n",
    "<li>min_samples_leaf: The minimum number of samples required to be at a leaf node. Default = 1.</li>\n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "https://en.wikipedia.org/wiki/Decision_tree_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Data\n",
    "X = train_data\n",
    "y = train_converted\n",
    "\n",
    "# Build classifier model with default parameters\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit model to all of the training data\n",
    "dtc.fit(X, y)\n",
    "\n",
    "# Mean accuracy of the training data and labels\n",
    "print 'Training data accuracy score', dtc.score(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = dtc.predict(test_data)\n",
    "print 'Example predictions', predictions[:5]\n",
    "\n",
    "# Mean accuracy of the test data and labels\n",
    "print 'Test data accuracy score', dtc.score(test_data, test_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "    \n",
    "tree.export_graphviz(dtc, out_file='tree.dot', feature_names=features)\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "\n",
    "src = graphviz.Source(dot_graph)\n",
    "src.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is the method of holding out a portion of your training data for model evaulation. There are many types of cross-validation, such as: k-fold, stratified k-fold, shuffle split.  \n",
    "\n",
    "The goals of cross-validation are:\n",
    "<li>Ensure that your model does not overfit the training data</li>\n",
    "<li>Select optimal model parameters</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Function used to print cross-validation scores\n",
    "def training_score(est, X, y, cv):\n",
    "    acc = cross_val_score(est, X, y, cv = cv, scoring='accuracy')\n",
    "    roc = cross_val_score(est, X, y, cv = cv, scoring='roc_auc')\n",
    "    print '5-fold Train CV | Accuracy:', round(np.mean(acc), 3),'+/-', \\\n",
    "    round(np.std(acc), 3),'| ROC AUC:', round(np.mean(roc), 3), '+/-', round(np.std(roc), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data\n",
    "X = train_data\n",
    "y = train_converted\n",
    "\n",
    "# build model\n",
    "dtc = tree.DecisionTreeClassifier(criterion='gini', max_features=None, max_depth=3, \n",
    "                                  min_samples_split = 10, min_samples_leaf=5)\n",
    "\n",
    "# cross-validation \n",
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "training_score(dtc, X, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model on all of the training data\n",
    "dtc.fit(X, y)\n",
    "\n",
    "# View the tree\n",
    "tree.export_graphviz(dtc, out_file='tree.dot', feature_names=features)\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "\n",
    "src = graphviz.Source(dot_graph)\n",
    "src.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Estimate the probability of a binary response based on one or more features. The probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "Benefits:\n",
    "<li>Fast</li>\n",
    "<li>Stable</li>\n",
    "<li>Can handle sparse data</li>\n",
    "\n",
    "Disadvantages:\n",
    "<li>Necessary to normalize data</li>\n",
    "<li>Data must be numeric</li>\n",
    "\n",
    "Parameters:\n",
    "<li>penalty: Used to specify the norm used in the penalization ('l1' or 'l2'). Default = 'l2'.</li>\n",
    "<li>C: Inverse of regularization strength, smaller values specify stronger regularization. It must be a positive float. Default = 1.0.</li>\n",
    "<li>tol: Tolerance for stopping criteria. Default = 0.0001.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "train_scaled = scaler.transform(train_data)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "from sklearn import linear_model\n",
    "X = train_scaled\n",
    "y = train_converted\n",
    "\n",
    "# Build model using default parameter values\n",
    "lrc = linear_model.LogisticRegression()\n",
    "\n",
    "# cross-validation \n",
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "training_score(lrc, X, y, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y)\n",
    "\n",
    "# Run classifier\n",
    "Y_pred = lrc.fit(X_train, Y_train).predict(X_test)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Model Goal: combine predictions of several models in order to improve the accuracy (decrease bias) and robustness (decrease variance) over a single model.\n",
    "\n",
    "Random Forest: combines de-correlated trees, where each tree is built from a bootstrap sample and node splits are calculated from random feature subsets.\n",
    "\n",
    "Parameters (in addition to decision tree parameters)\n",
    "<li>n_estimators: The number of trees in the forest.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = train_scaled\n",
    "y = train_converted\n",
    "\n",
    "# Build model\n",
    "rfc = RandomForestClassifier(criterion='gini', max_features=None, max_depth=3, \n",
    "                             min_samples_split = 10, min_samples_leaf=5,\n",
    "                             n_estimators = 100,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "# Cross-validation \n",
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "training_score(rfc, X, y, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Parameters that are not directly learnt within estimators can be set by searching a parameter space for the best cross-validated performance score. </li>\n",
    "<li>The drawback to grid search is that it can be computationally expensive and slow if you search a wide parameter space. </li>\n",
    "<li>Another parameter search option is randomized search - sampling from a distribution of possible parameter values. </li> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid Search Random Forest parameters\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = train_scaled\n",
    "y = train_converted\n",
    "\n",
    "# Perform a grid search on random forest parameters\n",
    "random_forest_grid = {'max_depth': [3, 5, None],\n",
    "                      'max_features': ['sqrt', 'log2', None],\n",
    "                      'min_samples_split': [10],\n",
    "                      'min_samples_leaf': [5],\n",
    "                      'n_estimators': [100],\n",
    "                      'random_state': [1]}\n",
    "\n",
    "rf_gridsearch = GridSearchCV(RandomForestClassifier(),\n",
    "                             random_forest_grid,\n",
    "                             n_jobs=-1,\n",
    "                             verbose=True,\n",
    "                             scoring='accuracy')\n",
    "rf_gridsearch.fit(X, y)\n",
    "\n",
    "print \"Best parameters:\", rf_gridsearch.best_params_\n",
    "print \"\"\n",
    "\n",
    "best_rf_model = rf_gridsearch.best_estimator_\n",
    "\n",
    "# Cross Validate the best model\n",
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "training_score(best_rf_model, X, y, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrain the best model on all of the training data\n",
    "best_rf_model.fit(X, y)\n",
    "\n",
    "# Calculate features importance, with higher values being more favorable\n",
    "f = best_rf_model.feature_importances_\n",
    "\n",
    "# Plot the feature importance\n",
    "f_index = np.argsort(f)\n",
    "sorted_features = f[f_index.tolist()]\n",
    "sorted_features_names = features[f_index]\n",
    "num_features = np.arange(1, (len(features)+1),1)\n",
    "    \n",
    "plt.bar(num_features, sorted_features, width=0.6, align='center',alpha=0.6)\n",
    "plt.xticks(num_features, sorted_features_names, rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Relative Feature Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.savefig('feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic (ROC) Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is generated by varying the cut-off threshold over the whole range of the classifier's output,\n",
    "thus generating a curve from many working points.\n",
    "\n",
    "ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis.\n",
    "This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero,\n",
    "and a true positive rate of one. This is not very realistic, but it does mean that a larger area\n",
    "under the curve (AUC) is usually better.\n",
    "\n",
    "The “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate \n",
    "while minimizing the false positive rate.\n",
    "\n",
    "The goal in ROC space is to be in the upper-left-hand corner.\n",
    "\n",
    "The best AUC value is 1 and random classifier would have a value of 0.5.\n",
    "\n",
    "This example shows the ROC response of different datasets, created from K-fold cross-validation. \n",
    "Taking all of these curves, it is possible to calculate the mean area under curve, and see the variance of \n",
    "the curve when the training set is split into different subsets. This roughly shows how the classifier output\n",
    "is affected by changes in the training data, and how different the splits generated by K-fold cross-validation\n",
    "are from one another.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "# Plot the ROC Curve\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tpr = dict()\n",
    "fpr = dict()\n",
    "roc_auc = dict()\n",
    "plt.figure(num=1, figsize=(8,8))\n",
    "\n",
    "X = train_scaled\n",
    "y = train_converted\n",
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    probas_ = best_rf_model.fit(X[train], y[train]).predict_proba(X[test])[:, 1]\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr[i], tpr[i], thresholds1 = roc_curve(y[test], probas_)\n",
    "    mean_tpr += sp.interp(mean_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i], tpr[i], lw=1, label='RF ROC fold %d (area = %0.2f)' % ((i+1), roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "\n",
    "mean_tpr /= len(cv)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "         label='RF Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate (Fall-out)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('Receiver operating characteristic curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision - Recall (PR) Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PR curves feature Precision on the Y axis, and Recall on the X axis.\n",
    "\n",
    "<li>Precision = TP/ TP + FP</li>\n",
    "<li>Recall = TP/ TP + FN</li>\n",
    "\n",
    "A high area under the PR curve represents both high recall and high precision, where high precision relates\n",
    "to a low false positive rate, and high recall relates to a low false negative rate. High scores for both\n",
    "show that the classifier is returning accurate results (high precision), as well as returning a majority\n",
    "of all positive results (high recall).\n",
    "\n",
    "When dealing with highly skewed classes, the PR curve is more informative than the ROC curve. \n",
    "\n",
    "The goal in PR space is to be in the upper-right-hand corner.\n",
    "\n",
    "The best AUC value is 1 and the worst value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "pr_auc = dict()\n",
    "mean_recall = 0.0\n",
    "mean_precision = np.linspace(0, 1, 100)\n",
    "plt.figure(num=1, figsize=(8,8))\n",
    "\n",
    "X = train_scaled\n",
    "y = train_converted\n",
    "cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    probas_ = best_rf_model.fit(X[train], y[train]).predict_proba(X[test])[:, 1]\n",
    "    # Compute PR curve and area under the curve\n",
    "    precision[i], recall[i], thresholds2 = precision_recall_curve(y[test], probas_)\n",
    "    mean_recall += sp.interp(mean_precision, precision[i], recall[i])\n",
    "    pr_auc[i] = average_precision_score(y[test], probas_)\n",
    "    plt.plot(precision[i], recall[i], lw=1, label='RF PR fold %d (area = %0.2f)' % ((i+1), pr_auc[i]))\n",
    "\n",
    "mean_recall /= len(cv)\n",
    "mean_recall[-1] = 0.0\n",
    "mean_auc = auc(mean_precision, mean_recall)\n",
    "plt.plot(mean_precision, mean_recall, 'k--',\n",
    "         label='RF Mean PR (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('True Positive Rate (Recall)')\n",
    "plt.ylabel('Positive Predicitive Value (Precision)')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "def save_model(mdl, path):\n",
    "    with open(path, 'w') as f:\n",
    "        pickle.dump(mdl, f)\n",
    "\n",
    "# pickle model\n",
    "pickle_path = 'best_model.pkl'\n",
    "mdl = best_rf_model.fit(X, y)\n",
    "save_model(mdl, pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(pickle_path, 'r') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "# Make predictions\n",
    "predictions = model.predict(test_scaled)\n",
    "print 'Test Accuracy', accuracy_score(test_converted, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
